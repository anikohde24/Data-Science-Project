# -*- coding: utf-8 -*-
"""Emotion Rnn

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WAZ5s2QPpyojRrparsxbpyCRT4Tc32EV
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# from wordcloud import WordCloud

import nltk
nltk.download("punkt")
nltk.download("wordnet")
nltk.download("stopwords")

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing import sequence #unique id

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, SimpleRNN, Dropout, Embedding
import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv("/content/Emotion_final.csv")

df

df['Emotion'].value_counts()

df['Emotion'].replace({"happy":0,"sadness":1,"anger":2,"fear":3,"love":4,"surprise":5},inplace=True)

df

df.isnull().sum()

def cleantext(text):
  tokens = word_tokenize(text.lower())
  ftoken = [t for t in tokens if(t.isalpha())]
  stop = stopwords.words("english")
  ctoken = [t for t in ftoken if(t not in stop)]
  lemma = WordNetLemmatizer()
  ltoken = [lemma.lemmatize(t) for t in ctoken]
  return " ".join(ltoken)

df["clean_text"]=df["Text"].apply(cleantext)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df["Emotion"] = le.fit_transform(df["Emotion"])

df

x = df["clean_text"]
y = df["Emotion"]

from sklearn.model_selection import train_test_split
xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, random_state=1)

sentlen = []

for sent in df["clean_text"]:
  sentlen.append(len(word_tokenize(sent)))

df["SentLen"] = sentlen 
df.head()

max(sentlen)

np.quantile(sentlen,0.95)

max_len = np.quantile(sentlen, 0.95)

tok = Tokenizer(char_level=False, split=" ")
#char_level	if True, every character will be treated as a token.

tok.fit_on_texts(xtrain)
tok.index_word

vocab_len = len(tok.index_word)
vocab_len

seqtrain = tok.texts_to_sequences(xtrain) #step1
seqtrain

seqmattrain = sequence.pad_sequences(seqtrain, maxlen= int(max_len)) #step2
seqmattrain

seqtest = tok.texts_to_sequences(xtest)
seqmattest = sequence.pad_sequences(seqtest, maxlen=int(max_len))

seqmattest

vocab_len

rnn = Sequential()

rnn.add(Embedding(vocab_len+1,15, input_length=int(max_len), mask_zero=True))
rnn.add(SimpleRNN(units=32, activation="tanh"))
rnn.add(Dense(units=32, activation="relu"))
rnn.add(Dropout(0.2))

rnn.add(Dense(units=6, activation="softmax"))

rnn.compile(optimizer="adam", loss="sparse_categorical_crossentropy",metrics=['accuracy'])

history=rnn.fit(seqmattrain, ytrain, validation_split=0.2, epochs=10)

seqmattest[0]

yprob=rnn.predict(seqmattest)
yprob[0]

ypred=yprob.argmax(axis=1)
ypred

from sklearn.metrics import classification_report
print(classification_report(ytest,ypred))

