# -*- coding: utf-8 -*-
"""Disaster Tweets  (Rnn on Emoji)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RlqFptaSr-SpzC3s2lrJynggOh_ujOYd
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

import nltk
nltk.download("punkt")
nltk.download("wordnet")
nltk.download("stopwords")

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing import sequence #unique id

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, SimpleRNN, Dropout, Embedding

df = pd.read_csv('/content/tweets.csv')

df

df['keyword'].value_counts()

df['target'].value_counts()

df.isnull().sum()

df['location'].value_counts()

df.drop(['id'],axis=1,inplace=True)
df.drop(['keyword'],axis=1,inplace=True)
df.drop(['location'],axis=1,inplace=True)

df.head()

!pip install emot

import re
import pickle
import emot
from emot.emo_unicode import UNICODE_EMOJI
from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO

emot_obj = emot.core.emot()

emot_obj.bulk_emoji(df['text'])

def clean_text(text):
  token = word_tokenize(text.lower()) #case conversion and tokenization

  #non alphabet removal
  ftoken=[i for i in token if i.isalpha()]

  #stop word removal
  stwd = stopwords.words("english")
  stoken = [i for i in ftoken if i not in stwd]

  #lemma
  lemma = WordNetLemmatizer()
  ltoken = [lemma.lemmatize(i) for i in stoken]

  #joining list of mess
  return " ".join(ltoken)

import nltk
nltk.download('omw-1.4')

df['cleam_msg'] = df['text'].apply(clean_text)

df.head()

emot_obj.bulk_emoji(df['cleam_msg'])

x = df['cleam_msg']
y = df['target']

xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, random_state=1)

sentlen = []

for sent in df["cleam_msg"]:
  sentlen.append(len(word_tokenize(sent)))

df["SentLen"] = sentlen 
df.head()

max_len = np.quantile(sentlen, 0.95)

tok = Tokenizer(char_level=False, split=" ")
#char_level	if True, every character will be treated as a token.

tok.fit_on_texts(xtrain)
tok.index_word

vocab_len = len(tok.index_word)
vocab_len

seqtrain = tok.texts_to_sequences(xtrain) #step1
seqtrain

seqmattrain = sequence.pad_sequences(seqtrain, maxlen= int(max_len)) #step2
seqmattrain

seqtest = tok.texts_to_sequences(xtest)
seqmattest = sequence.pad_sequences(seqtest, maxlen=int(max_len))

rnn = Sequential()

rnn.add(Embedding(vocab_len+1,700, input_length=int(max_len), mask_zero=True))
rnn.add(SimpleRNN(units=32, activation="tanh"))
rnn.add(Dense(units=32, activation="relu"))
rnn.add(Dropout(0.2))

rnn.add(Dense(units=1, activation="sigmoid"))

rnn.compile(optimizer="adam", loss="binary_crossentropy")

rnn.fit(seqmattrain, ytrain, batch_size=50, epochs=25)

ypred = rnn.predict(seqmattest)

ypred = ypred>0.5

from sklearn.metrics import classification_report

print(classification_report(ytest,ypred))

